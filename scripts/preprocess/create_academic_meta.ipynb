{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfiles(origin_paths, img_paths):\n",
    "    for origin_path, img_path in zip(origin_paths, img_paths):\n",
    "        if not os.path.exists(img_path):\n",
    "            try:\n",
    "                copyfile(origin_path, img_path)\n",
    "            except:\n",
    "                print('Error copying file: ', origin_path)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forgery Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2000\n",
    "DATA_ROOT = '/Datasets/DeepfakeAttribution/release'\n",
    "\n",
    "FORGERY_TYPES = {\n",
    "    \"RealFace\": 0,\n",
    "    \"IdentitySwap\": 1,\n",
    "    \"ExpressionTransfer\": 2,\n",
    "    \"AttributeManipulation\": 3,\n",
    "    \"EntireFaceSyncthesis\": 4,\n",
    "}\n",
    "\n",
    "FORGERYID_TO_TYPE = {i: k for k, i in FORGERY_TYPES.items()}\n",
    "\n",
    "ATTACK_METHOD = {\n",
    "    \"Real\": 0,\n",
    "    \"FaceSwap\": 1,\n",
    "    \"Deepfakes\": 2,\n",
    "    \"FaceShifter\": 3,\n",
    "    \"DeepFaceLab\": 4,\n",
    "    \"FSGAN\": 5,\n",
    "    \"FaceNet\": 6,\n",
    "    \"Face2Face\": 7,\n",
    "    \"NeuralTextures\": 8,\n",
    "    \"Talking-Head-Video\": 9,\n",
    "    \"ATVG-Net\": 10,\n",
    "    \"FOMM\": 11,\n",
    "    \"Wav2Lip\": 12,\n",
    "    \"ATFHP\": 13,\n",
    "    \"MakeItTalk\": 14,\n",
    "    \"MaskGAN\": 15,\n",
    "    \"StarGAN2\": 16,\n",
    "    \"SC-FEGAN\": 17,\n",
    "    \"DiscoFaceGAN\": 18,\n",
    "    \"FaceAPP\": 19,\n",
    "    \"StarGAN\": 20,\n",
    "    \"PGGAN\": 21,\n",
    "    \"CycleGAN\": 22,\n",
    "    \"StyleGAN\": 23,\n",
    "    \"StyleGAN2\": 24,\n",
    "}\n",
    "\n",
    "ATTACK_CODE = {\n",
    "    \"Real\": \"None\",\n",
    "    \"FaceSwap\": \"https://github.com/MarekKowalski/FaceSwap/\",\n",
    "    \"Deepfakes\": \"https://github.com/deepfakes/faceswap\",\n",
    "    \"FaceShifter\": \"https://github.com/mindslab-ai/faceshifter\",\n",
    "    \"DeepFaceLab\": \"https://github.com/iperov/DeepFaceLab\",\n",
    "    \"FSGAN\": \"https://github.com/YuvalNirkin/fsgan\",\n",
    "    \"FaceNet\": \"https://github.com/davidsandberg/facenet\",\n",
    "    \"Face2Face\": \"None\",\n",
    "    \"NeuralTextures\": \"https://github.com/SSRSGJYD/NeuralTexture\",\n",
    "    \"Talking-Head-Video\": \"https://github.com/sibozhang/Text2Video\",\n",
    "    \"ATVG-Net\": \"https://github.com/lelechen63/ATVGnet\",\n",
    "    \"FOMM\": \"https://github.com/AliaksandrSiarohin/first-order-model\",\n",
    "    \"Wav2Lip\": \"https://github.com/Rudrabha/Wav2Lip\",\n",
    "    \"ATFHP\": \"https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose\",\n",
    "    \"MakeItTalk\": \"https://github.com/adobe-research/MakeItTalk\",\n",
    "    \"MaskGAN\": \"https://github.com/switchablenorms/CelebAMask-HQ\",\n",
    "    \"StarGAN2\": \"https://github.com/clovaai/stargan-v2\",\n",
    "    \"SC-FEGAN\": \"https://github.com/run-youngjoo/SC-FEGAN\",\n",
    "    \"DiscoFaceGAN\": \"https://github.com/microsoft/DiscoFaceGAN\",\n",
    "    \"FaceAPP\": \"https://faceapp.com/app\",\n",
    "    \"StarGAN\": \"https://github.com/yunjey/stargan\",\n",
    "    \"PGGAN\": \"https://github.com/tkarras/progressive_growing_of_gans\",\n",
    "    \"CycleGAN\": \"https://github.com/junyanz/CycleGAN/\",\n",
    "    \"StyleGAN\": \"https://github.com/NVlabs/stylegan\",\n",
    "    \"StyleGAN2\": \"https://github.com/NVlabs/stylegan2\",\n",
    "}\n",
    "\n",
    "FORGERY_ATTACK = {\n",
    "    \"RealFace\": [\n",
    "        \"Real\",\n",
    "    ],\n",
    "    \"IdentitySwap\": [\n",
    "        \"FaceSwap\",\n",
    "        \"Deepfakes\",\n",
    "        \"FaceShifter\",\n",
    "        \"DeepFaceLab\",\n",
    "        \"FSGAN\",\n",
    "        \"FaceNet\",\n",
    "    ],\n",
    "    \"ExpressionTransfer\": [\n",
    "        \"Face2Face\",\n",
    "        \"NeuralTextures\",\n",
    "        \"Talking-Head-Video\",\n",
    "        \"ATVG-Net\",\n",
    "        \"FOMM\",\n",
    "        \"Wav2Lip\",\n",
    "        \"ATFHP\",\n",
    "        \"MakeItTalk\",\n",
    "    ],\n",
    "    \"AttributeManipulation\": [\n",
    "        \"MaskGAN\",\n",
    "        \"StarGAN2\",\n",
    "        \"SC-FEGAN\",\n",
    "        \"DiscoFaceGAN\",\n",
    "        \"FaceAPP\",\n",
    "        \"StarGAN\",\n",
    "    ],\n",
    "    \"EntireFaceSyncthesis\": [\n",
    "        \"PGGAN\",\n",
    "        \"CycleGAN\",\n",
    "        \"StyleGAN\",\n",
    "        \"StyleGAN2\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "ATTACK_TO_FORGERY = {att: \"\" for att in ATTACK_METHOD}\n",
    "for forgery in FORGERY_ATTACK:\n",
    "    for att in FORGERY_ATTACK[forgery]:\n",
    "        ATTACK_TO_FORGERY[att] = forgery\n",
    "\n",
    "forgerynet_label2method = {\n",
    "\t1: 'FaceShifter',\n",
    "\t2: 'FSGAN',\n",
    "\t3: 'DeepFaceLab',\n",
    "\t4: 'BlendFace',\n",
    "\t5: 'MMReplacement',\n",
    "\t6: 'DeepFakes-StarGAN-Stack',\n",
    "\t7: 'Talking-Head-Video',\n",
    "\t8: 'ATVG-Net',\n",
    "\t9: 'StarGAN-BlendFace-Stack',\n",
    "   10: 'FOMM',\n",
    "   11: 'StyleGAN2',\n",
    "   12: 'MaskGAN',\n",
    "   13: 'StarGAN2',\n",
    "   14: 'SC-FEGAN',\n",
    "   15: 'DiscoFaceGAN',    \n",
    "}\n",
    "\n",
    "ATTACK_METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FaceForensics++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Datasets/deepfakes_detection_datasets/faceforensics/ffpp_video'\n",
    "compressions = [\n",
    "    'c23', \n",
    "    # 'c40',\n",
    "]\n",
    "attack_types = [\n",
    "    'Deepfakes',\n",
    "    'Face2Face',\n",
    "    'FaceSwap',\n",
    "    'NeuralTextures',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 100\n",
    "\n",
    "workers = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for compression in compressions:\n",
    "        origin_paths = []\n",
    "        img_paths = []\n",
    "        attack_type = 'Real'\n",
    "        video_dirs = glob(f'{data_dir}/original_sequences/youtube/c23/images_v3/*')\n",
    "        prefix = f'{DATA_ROOT}/{ATTACK_TO_FORGERY[attack_type]}/{attack_type}/faceforensics'\n",
    "        os.makedirs(prefix, exist_ok=True)\n",
    "        for video_dir in video_dirs:\n",
    "            sorted_images_names = np.array(sorted(os.listdir(video_dir), key=lambda x: int(x.split('.')[0])))\n",
    "            ind = np.linspace(0, len(sorted_images_names) - 1, num_frames, endpoint=True, dtype=int)\n",
    "            sub_img_paths = [os.path.join(video_dir, x) for x in sorted_images_names[ind]]\n",
    "            for src in sub_img_paths:\n",
    "                origin_paths.append(src)\n",
    "                dst = f'{prefix}/{compression}-{\"-\".join(src.split(\"/\")[-2:])}'\n",
    "                img_paths.append(dst)\n",
    "        workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "        print(compression, attack_type, len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "    'compression': [],\n",
    "    'frame_idx': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(img_paths))):\n",
    "    img_path = img_paths[i]\n",
    "    origin_img_path = origin_paths[i]\n",
    "    attack, method, _, img_full_name = img_path.split('/')[-4:]\n",
    "    compression, img_name, frame_idx = img_full_name.split('-')\n",
    "    frame_idx = frame_idx.split('.')[0]\n",
    "    label = ATTACK_METHOD[method]\n",
    "    code = ATTACK_CODE[method]\n",
    "    image_source = 'FaceForensics++'\n",
    "    for key in data.keys():\n",
    "        data[key].append(eval(key))\n",
    "ffpp_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffpp_df = ffpp_df.drop_duplicates(subset=['img_path'])\n",
    "ffpp_c23_df = ffpp_df[ffpp_df.compression == 'c23']\n",
    "ffpp_c40_df = ffpp_df[ffpp_df.compression == 'c40']\n",
    "ffpp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffpp_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_real_meta.csv', index=False)\n",
    "ffpp_c23_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_c23_real_meta.csv', index=False)\n",
    "ffpp_c40_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_c40_real_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 10\n",
    "\n",
    "all_origin_paths = []\n",
    "all_img_paths = []\n",
    "\n",
    "workers = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for compression in compressions:\n",
    "        for attack_type in attack_types:\n",
    "            origin_paths = []\n",
    "            img_paths = []\n",
    "            video_dirs = glob(f'{data_dir}/manipulated_sequences/{attack_type}/{compression}/images_v3/*')\n",
    "            prefix = f'{DATA_ROOT}/{ATTACK_TO_FORGERY[attack_type]}/{attack_type}/faceforensics'\n",
    "            print(prefix)\n",
    "            os.makedirs(prefix, exist_ok=True)\n",
    "            for video_dir in video_dirs:\n",
    "                sorted_images_names = np.array(sorted(os.listdir(video_dir), key=lambda x: int(x.split('.')[0])))\n",
    "                ind = np.linspace(0, len(sorted_images_names) - 1, num_frames, endpoint=True, dtype=int)\n",
    "                sub_img_paths = [os.path.join(video_dir, x) for x in sorted_images_names[ind]]\n",
    "                for src in sub_img_paths:\n",
    "                    origin_paths.append(src)\n",
    "                    dst = f'{prefix}/{compression}-{\"-\".join(src.split(\"/\")[-2:])}'\n",
    "                    img_paths.append(dst)\n",
    "            workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "            all_origin_paths.extend(origin_paths)\n",
    "            all_img_paths.extend(img_paths)\n",
    "            print(compression, attack_type, len(all_origin_paths))\n",
    "\n",
    "for worker in workers:\n",
    "    worker.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "    'compression': [],\n",
    "    'frame_idx': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(all_origin_paths))):\n",
    "    img_path = all_img_paths[i]\n",
    "    origin_img_path = all_origin_paths[i]\n",
    "    attack, method, _, img_full_name = img_path.split('/')[-4:]\n",
    "    compression, img_name, frame_idx = img_full_name.split('-')\n",
    "    frame_idx = frame_idx.split('.')[0]\n",
    "    label = ATTACK_METHOD[method]\n",
    "    code = ATTACK_CODE[method]\n",
    "    image_source = 'FaceForensics++'\n",
    "    for key in data.keys():\n",
    "        data[key].append(eval(key))\n",
    "\n",
    "ffpp_df = pd.DataFrame(data)\n",
    "ffpp_c23_df = ffpp_df[ffpp_df.compression == 'c23']\n",
    "ffpp_c40_df = ffpp_df[ffpp_df.compression == 'c40']\n",
    "ffpp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffpp_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_meta.csv', index=False)\n",
    "ffpp_c23_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_c23_meta.csv', index=False)\n",
    "ffpp_c40_df.to_csv(f'{DATA_ROOT}/meta_data/ffpp_large_c40_meta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celeb-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Datasets/deepfakes_detection_datasets/celebdfv2'\n",
    "real_types = [\n",
    "    'Celeb-real',\n",
    "    # 'YouTube-real',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 50\n",
    "workers = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for real_type in real_types:\n",
    "        origin_paths = []\n",
    "        img_paths = []\n",
    "        attack_type = 'Real'\n",
    "        video_dirs = glob(f'{data_dir}/images_v1/{real_type}/*')\n",
    "        prefix = f'{DATA_ROOT}/{ATTACK_TO_FORGERY[attack_type]}/{attack_type}/CelebDF'\n",
    "        os.makedirs(prefix, exist_ok=True)\n",
    "        for video_dir in video_dirs:\n",
    "            sorted_images_names = np.array(sorted(os.listdir(video_dir), key=lambda x: int(x.split('.')[0])))\n",
    "            ind = np.linspace(0, len(sorted_images_names) - 1, num_frames, endpoint=True, dtype=int)\n",
    "            sub_img_paths = [os.path.join(video_dir, x) for x in sorted_images_names[ind]]\n",
    "            for src in sub_img_paths:\n",
    "                origin_paths.append(src)\n",
    "                dst = f'{prefix}/{real_type}-{\"-\".join(src.split(\"/\")[-2:])}'\n",
    "                img_paths.append(dst)\n",
    "        workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "        print(real_type, attack_type, len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "    'source': [],\n",
    "    'frame_idx': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(img_paths))):\n",
    "    img_path = img_paths[i]\n",
    "    origin_img_path = origin_paths[i]\n",
    "    attack, method, _, img_full_name = img_path.split('/')[-4:]\n",
    "    source, _, img_name, frame_idx = img_full_name.split('-')\n",
    "    frame_idx = frame_idx.split('.')[0]\n",
    "    label = ATTACK_METHOD[method]\n",
    "    code = ATTACK_CODE[method]\n",
    "    image_source = 'CelebDF'\n",
    "    for key in data.keys():\n",
    "        data[key].append(eval(key))\n",
    "celeb_df = pd.DataFrame(data)\n",
    "celeb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_df = celeb_df.drop_duplicates(subset=['img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_df.to_csv(f'{DATA_ROOT}/meta_data/celebdf_large_real_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForgeryNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Datasets/deepfakes_detection_datasets/ForgeryNet/unzip_files/Training'\n",
    "image_list_path = os.path.join(data_dir, 'image_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "lines = open(image_list_path).read().splitlines()\n",
    "class2paths = {}\n",
    "for line in tqdm(lines):\n",
    "    img_path = os.path.join(data_dir, 'images', line.split(' ')[0])\n",
    "    binary_cls_label, triple_cls_label, cls16_label = map(int, line.split()[-3:])\n",
    "    if cls16_label not in class2paths:\n",
    "        class2paths[cls16_label] = []\n",
    "    class2paths[cls16_label].append(img_path)\n",
    "class2paths = collections.OrderedDict(sorted(class2paths.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in class2paths.keys():\n",
    "    print(label, len(class2paths[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "    'frame_idx': [],\n",
    "}\n",
    "\n",
    "all_origin_paths = []\n",
    "all_img_paths = []\n",
    "\n",
    "workers = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for ori_label in class2paths.keys():\n",
    "        if ori_label in [0, 4, 5, 6, 9, 15]:\n",
    "            continue\n",
    "\n",
    "        origin_paths = []\n",
    "        img_paths = []\n",
    "\n",
    "        method = forgerynet_label2method[ori_label]\n",
    "        attack = ATTACK_TO_FORGERY[method]\n",
    "        label = ATTACK_METHOD[method]\n",
    "        code = ATTACK_CODE[method]\n",
    "        image_source = 'ForgeryNet'\n",
    "\n",
    "        prefix = f'{DATA_ROOT}/{attack}/{method}/{image_source}'\n",
    "        chosen_img_paths = class2paths[ori_label]\n",
    "        np.random.seed(SEED)\n",
    "        chosen_img_paths = np.random.choice(chosen_img_paths, 10000, replace=False)\n",
    "\n",
    "        for img_path in tqdm(chosen_img_paths):\n",
    "            img_name = img_path.split('/')[-1]\n",
    "            origin_img_path = img_path\n",
    "            origin_paths.append(origin_img_path)\n",
    "            frame_idx = int(img_name.split('.')[0].replace('frame', ''))\n",
    "            img_path = f'{prefix}/{\"-\".join(img_path.split(\"/\")[-4:])}'\n",
    "            os.makedirs(os.path.dirname(img_path), exist_ok=True)\n",
    "            img_paths.append(img_path)\n",
    "            for key in data.keys():\n",
    "                data[key].append(eval(key))\n",
    "        workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "        all_origin_paths.extend(origin_paths)\n",
    "        all_img_paths.extend(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in workers:\n",
    "    print(worker.done())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forgerynet_df = pd.DataFrame(data)\n",
    "forgerynet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forgerynet_df.to_csv(f'{DATA_ROOT}/meta_data/forgerynet_large_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiverseFakeFaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Datasets/deepfakes_detection_datasets/DiverseFakeFaceDataset'\n",
    "attack_types = {\n",
    "    'faceapp': 'FaceAPP',\n",
    "    'pggan_v2': 'PGGAN',\n",
    "    'stylegan_ffhq': 'StyleGAN',\n",
    "    'stargan': 'StarGAN',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_dir}/all_img_paths.txt') as f:\n",
    "    origin_img_paths = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_group = {\n",
    "    'FaceAPP': [],\n",
    "    'PGGAN': [],\n",
    "    'StyleGAN': [],\n",
    "    'StarGAN': [],\n",
    "}\n",
    "\n",
    "rest_img_group = {\n",
    "    'FaceAPP': [],\n",
    "    'PGGAN': [],\n",
    "    'StyleGAN': [],\n",
    "    'StarGAN': [],\n",
    "}\n",
    "\n",
    "for img_path in tqdm(origin_img_paths):\n",
    "    if 'mask' in img_path.split('/')[-2]:\n",
    "        continue\n",
    "    attack_name = img_path.split('/')[-3]\n",
    "    if attack_name == 'ffhq':\n",
    "        continue\n",
    "    attack = attack_types[attack_name]\n",
    "    if 'train' not in img_path.split('/')[-2]:\n",
    "        rest_img_group[attack].append(img_path)\n",
    "        continue\n",
    "    img_group[attack].append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attack in img_group.keys():\n",
    "    print(attack, len(img_group[attack]))\n",
    "    print(attack, len(rest_img_group[attack]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "img_group['FaceAPP'] = random.sample(rest_img_group['FaceAPP'], 10000 - len(img_group['FaceAPP'])) + img_group['FaceAPP']\n",
    "img_group['PGGAN'] = random.sample(rest_img_group['PGGAN'], 10000 - len(img_group['PGGAN'])) + img_group['PGGAN']\n",
    "img_group['StarGAN'] = random.sample(rest_img_group['StarGAN'], 10000 - len(img_group['StarGAN'])) + img_group['StarGAN']\n",
    "img_group['StyleGAN'] = random.sample(rest_img_group['StyleGAN'], 10000 - len(img_group['StyleGAN'])) + img_group['StyleGAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attack in img_group.keys():\n",
    "    print(attack, len(img_group[attack]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_origin_paths = []\n",
    "all_img_paths = []\n",
    "\n",
    "workers = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for attack in img_group:\n",
    "        origin_paths = []\n",
    "        img_paths = []\n",
    "        for img_path in tqdm(img_group[attack]):\n",
    "            prefix = f'{DATA_ROOT}/{ATTACK_TO_FORGERY[attack]}/{attack}/DFFD'\n",
    "            os.makedirs(prefix, exist_ok=True)\n",
    "            dst = f'{prefix}/{\"-\".join(img_path.split(\"/\")[-2:])}'\n",
    "            origin_paths.append(img_path)\n",
    "            img_paths.append(dst)\n",
    "        workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "        all_origin_paths.extend(origin_paths)\n",
    "        all_img_paths.extend(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(all_origin_paths))):\n",
    "    img_path = all_img_paths[i]\n",
    "    origin_img_path = all_origin_paths[i]\n",
    "    attack, method, _, img_name = img_path.split('/')[-4:]\n",
    "    label = ATTACK_METHOD[method]\n",
    "    code = ATTACK_CODE[method]\n",
    "    image_source = 'DFFD'\n",
    "    for key in data.keys():\n",
    "        data[key].append(eval(key))\n",
    "dffd_df = pd.DataFrame(data)\n",
    "dffd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffd_df.to_csv(f'{DATA_ROOT}/meta_data/dffd_large_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForgeryNIR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Datasets/deepfakes_detection_datasets/ForgeryNIR'\n",
    "attack_types = {\n",
    "    'cyclegan': 'CycleGAN',\n",
    "    'progan': 'PGGAN',\n",
    "    'stylegan': 'StyleGAN',\n",
    "    'stylegan2': 'StyleGAN2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_dir}/unzip_files/lists/ForgeryNIR-mix_multi.txt') as f:\n",
    "    origin_img_paths = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_group = {\n",
    "    'CycleGAN': [],\n",
    "    'PGGAN': [],\n",
    "    'StyleGAN': [],\n",
    "    'StyleGAN2': [],\n",
    "}\n",
    "\n",
    "for img_path in tqdm(origin_img_paths):\n",
    "    attack_name = img_path.split('/')[-2]\n",
    "    attack = attack_types[attack_name]\n",
    "    img_group[attack].append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "img_group['CycleGAN'] = random.sample(img_group['CycleGAN'], 10000)\n",
    "img_group['PGGAN'] = random.sample(img_group['PGGAN'], 10000)\n",
    "img_group['StyleGAN'] = random.sample(img_group['StyleGAN'], 10000)\n",
    "img_group['StyleGAN2'] = random.sample(img_group['StyleGAN2'], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_origin_paths = []\n",
    "all_img_paths = []\n",
    "\n",
    "workers = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for attack in img_group:\n",
    "        origin_paths = []\n",
    "        img_paths = []\n",
    "        for img_path in tqdm(img_group[attack]):\n",
    "            prefix = f'{DATA_ROOT}/{ATTACK_TO_FORGERY[attack]}/{attack}/ForgeryNIR'\n",
    "            os.makedirs(prefix, exist_ok=True)\n",
    "            src = f'{data_dir}/unzip_files/{img_path}'\n",
    "            dst = f'{prefix}/{img_path.split(\"/\")[-1]}'\n",
    "            origin_paths.append(src)\n",
    "            img_paths.append(dst)\n",
    "        workers.append(executor.submit(copyfiles, origin_paths, img_paths))\n",
    "        all_origin_paths.extend(origin_paths)\n",
    "        all_img_paths.extend(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack': [], \n",
    "    'img_name': [], \n",
    "    'label': [], \n",
    "    'method': [],\n",
    "    'code': [], \n",
    "    'image_source': [],\n",
    "    'img_path': [],\n",
    "    'origin_img_path': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(all_origin_paths))):\n",
    "    img_path = all_img_paths[i]\n",
    "    origin_img_path = all_origin_paths[i]\n",
    "    attack, method, _, img_name = img_path.split('/')[-4:]\n",
    "    label = ATTACK_METHOD[method]\n",
    "    code = ATTACK_CODE[method]\n",
    "    image_source = 'ForgeryNIR+'\n",
    "    for key in data.keys():\n",
    "        data[key].append(eval(key))\n",
    "forgeryNIR_df = pd.DataFrame(data)\n",
    "forgeryNIR_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forgeryNIR_df.to_csv(f'{DATA_ROOT}/meta_data/forgerynir_large_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_files = [\n",
    "    'ffpp_c23_meta.csv',\n",
    "    # 'ffpp_c40_meta.csv',\n",
    "    # 'ffpp_meta.csv',\n",
    "    'ffpp_c23_real_meta.csv',\n",
    "    # 'ffpp_c40_real_meta.csv',\n",
    "    # 'ffpp_meta_real.csv',\n",
    "    'celebdf_real_meta.csv',\n",
    "    'forgerynet_meta.csv',\n",
    "    'dffd_meta.csv',\n",
    "    'forgerynir_meta.csv',\n",
    "]\n",
    "\n",
    "all_dfs = []\n",
    "for meta_file in meta_files:\n",
    "    meta_path = os.path.join(DATA_ROOT, 'meta_data', meta_file)\n",
    "    df = pd.read_csv(meta_path)\n",
    "    all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merge_df['forgery_label'] = merge_df['attack'].apply(lambda x: FORGERY_TYPES[x])\n",
    "merge_df['forgery_type'] = merge_df['attack']\n",
    "merge_df['tag'] = 1\n",
    "merge_df['face_type'] = merge_df['method'].apply(lambda x: 1 if x == 'Real' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"img_name\", \"label\", \"method\", \"forgery_type\", \"forgery_label\", \"face_type\", \"tag\",\n",
    "        \"code\", \"image_source\", \"img_path\", \"compression\", \"frame_idx\"]\n",
    "merge_df = merge_df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df.to_csv(f'{DATA_ROOT}/meta_data/fake_val_merge_meta.csv', index=False)\n",
    "merge_df.to_csv(f'{DATA_ROOT}/meta_data/real_fake_val_merge_meta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_files = [\n",
    "    'ffpp_large_c23_meta.csv',\n",
    "    # 'ffpp_c40_large_meta.csv',\n",
    "    # 'ffpp_large_meta.csv',\n",
    "    'ffpp_large_c23_real_meta.csv',\n",
    "    # 'ffpp_c40_large_real_meta.csv',\n",
    "    # 'ffpp_meta_large_real.csv',\n",
    "    'celebdf_large_real_meta.csv',\n",
    "    'forgerynet_large_meta.csv',\n",
    "    # 'fakeavceleb_meta.csv',\n",
    "    'dffd_large_meta.csv',\n",
    "    'forgerynir_large_meta.csv',\n",
    "    # 'aisc_meta.csv',\n",
    "]\n",
    "\n",
    "all_dfs = []\n",
    "for meta_file in meta_files:\n",
    "    meta_path = os.path.join(DATA_ROOT, 'meta_data', meta_file)\n",
    "    df = pd.read_csv(meta_path)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merge_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merge_df['forgery_label'] = merge_df['attack'].apply(lambda x: FORGERY_TYPES[x])\n",
    "merge_df['forgery_type'] = merge_df['attack']\n",
    "merge_df['tag'] = 1\n",
    "merge_df['face_type'] = merge_df['method'].apply(lambda x: 1 if x == 'Real' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"img_name\", \"label\", \"method\", \"forgery_type\", \"forgery_label\", \"face_type\", \"tag\",\n",
    "        \"code\", \"image_source\", \"img_path\", \"compression\", \"frame_idx\"]\n",
    "merge_df = merge_df[order]\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df.to_csv(f'{DATA_ROOT}/meta_data/fake_large_merge_meta.csv', index=False)\n",
    "merge_df.to_csv(f'{DATA_ROOT}/meta_data/real_fake_large_merge_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = f'{DATA_ROOT}/meta_data/real_fake_large_merge_meta.csv'\n",
    "semi_df = pd.read_csv(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_sup(df):\n",
    "    if df['image_source'] == 'FaceForensics++':\n",
    "        return True\n",
    "    elif df['image_source'] == 'CelebDF':\n",
    "        return True\n",
    "    elif df['image_source'] == 'ForgeryNet':\n",
    "        return df['method'] in [\n",
    "            'FaceShifter',\n",
    "            'DeepFaceLab',\n",
    "            'FSGAN',\n",
    "            'Talking-Head-Video',\n",
    "            'ATVG-Net',\n",
    "            'FOMM',\n",
    "            'MaskGAN',\n",
    "            'StarGAN2',\n",
    "            'SC-FEGAN',\n",
    "            'StyleGAN2',\n",
    "        ]\n",
    "    elif df['image_source'] == 'DFFD':\n",
    "        return True\n",
    "    elif df['image_source'] == 'ForgeryNIR+':\n",
    "        return df['method'] in [\n",
    "            'CycleGAN',\n",
    "            'StyleGAN2',\n",
    "        ]\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df = semi_df[semi_df.apply(semi_sup, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_sup_label(df):\n",
    "    if df['image_source'] == 'FaceForensics++':\n",
    "        if df['method'] in [\n",
    "            \"Deepfakes\",\n",
    "            \"Face2Face\",\n",
    "            'Real',\n",
    "        ]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif df['image_source'] == 'CelebDF':\n",
    "        if df['method'] in []:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif df['image_source'] == 'ForgeryNet':\n",
    "        if df['method'] in [\n",
    "            'DeepFaceLab',\n",
    "            'FOMM',\n",
    "            'MaskGAN',\n",
    "        ]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif df['image_source'] == 'DFFD':\n",
    "        if df['method'] in [\n",
    "            \"FaceAPP\",\n",
    "            \"StyleGAN\",\n",
    "        ]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif df['image_source'] == 'ForgeryNIR+':\n",
    "        if df['method'] in [\n",
    "            \"CycleGAN\",\n",
    "        ]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default unlabeled\n",
    "semi_df['tag'] = 2\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "for i, k in semi_df.method.groupby(semi_df.image_source).value_counts().index:\n",
    "    idx = semi_df[lambda x: x['method'] == k][lambda x: x['image_source'] == i].index\n",
    "    num = len(idx)\n",
    "    if semi_sup_label({ 'image_source': i, 'method': k }):\n",
    "        # Labeled\n",
    "        semi_df.loc[random.sample(list(idx), int(num * 0.75 + 0.5)), 'tag'] = 1\n",
    "    else:\n",
    "        # Unlabeled\n",
    "        if k == 'Real':\n",
    "            semi_df.loc[random.sample(list(idx), num - 25000), 'tag'] = 0\n",
    "        else:\n",
    "            semi_df.loc[random.sample(list(idx), num - int(num * 0.75 + 0.5)), 'tag'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 0\n",
    "\n",
    "for i, k in semi_df.method.groupby(semi_df.image_source).value_counts().index:\n",
    "    idx = semi_df[lambda x: x['method'] == k][lambda x: x['image_source'] == i].index\n",
    "    semi_df.loc[idx, 'label'] = label_id\n",
    "    label_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "labeled_key = semi_df[semi_df.tag == 1].label.value_counts().keys()\n",
    "unlabeled_key = semi_df[semi_df.tag == 2].label.value_counts().keys()\n",
    "\n",
    "n = 0\n",
    "for i in labeled_key:\n",
    "    label_map[i] = n\n",
    "    n += 1\n",
    "    \n",
    "for i in unlabeled_key:\n",
    "    if i in label_map:\n",
    "        continue\n",
    "    label_map[i] = n\n",
    "    n += 1\n",
    "\n",
    "for i in range(0,25):\n",
    "    if i in label_map:\n",
    "        continue\n",
    "    label_map[i] = n\n",
    "    n += 1\n",
    "\n",
    "semi_df.label = semi_df.label.apply(lambda x: label_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_key = semi_df[semi_df.tag == 1].label.value_counts().keys()\n",
    "labeled_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_key = semi_df[semi_df.tag == 2].label.value_counts().keys()\n",
    "unlabeled_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df.method.groupby(semi_df.label).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df.face_type.groupby(semi_df.tag).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_df.to_csv(f'{DATA_ROOT}/meta_data/openset_real_fake_large_merge_meta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dirs = glob(f'{DATA_ROOT}/*/*/*')\n",
    "for dir in image_dirs:\n",
    "    # count the number of images in the directory\n",
    "    num_images = len(glob(f'{dir}/*'))\n",
    "    print(f'{dir} has {num_images} images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change setting of Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = f'{DATA_ROOT}/meta_data/Protocol2_openset_real_fake_large_merge_meta.csv'\n",
    "df = pd.read_csv(meta_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.face_type.groupby(df.tag).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.face_type != 1]\n",
    "df.face_type.groupby(df.tag).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.apply(lambda x: x - 1 if x < 9 else x - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{DATA_ROOT}/meta_data/Protocol1_openset_fake_large_merge_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('asic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd57629fc529d471aa94dc57c1ab5fcf9e7886def4cc193d342274ba8155b55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
